% -*- TeX-master: "YFT2023.tex"; eval: (longlines-mode); fill-column: 100000 -*-

\section{Model description}
\label{sec:model_description]}

\subsection{General characteristics}

The model can be considered to consist of several components, (i) the dynamics of the fish population; (ii) the fishery dynamics; (iii) the dynamics of tagged fish populations (iv) the observation models for the data; (v) the parameter estimation procedure; and (vi) stock assessment interpretations. Detailed technical descriptions of components (i)--(iv) are given in \citet{hampton_spatially-disaggregated_2001} and \citet{kleiber_multifan-cl_2019}, and summarised below. In addition, we describe the procedures followed for estimating the parameters of the model and the way in which stock status conclusions are drawn relative to a series of reference points.

\subsection{Population dynamics}
\label{sec:model_population_dynamics}

The model partitions the population into spatial regions, depending on the specified spatial stratification, and 40 quarterly age-classes. The last age-class comprises a \enquote{plus group} in which mortality and other characteristics are assumed to be constant. The population is \enquote{monitored} in the model at quarterly time steps, extending through a time window of 1952--2021. The main population dynamics processes are as follows.

\subsubsection{Recruitment}
\label{sec:model_recruitment}

Recruitment is defined as the appearance of age-class 1 quarter fish (i.e. fish averaging $\sim$ 20 cm FL) in the population. Yellowfin tuna spawning does not typically follow a clear seasonal pattern, especially in the tropical waters where most of the spawning occurs. Rather it occurs sporadically all year and is thought to be influenced by food availability \citep{itano_reproductive_2000}. The assessment model assumed that recruitment occurs instantaneously at the beginning of each quarter. This is a discrete approximation of continuous recruitment, but provides sufficient flexibility to allow a range of variability to be incorporated into the estimates as appropriate.

The proportion of total recruitment occurring in each region was initially set relative to the variation in average regional catch and then estimated during the later phases of the fitting procedure. Time-series variability in this proportion was estimated within the model and allowed to vary in a relatively unconstrained fashion.

In recent assessments of tuna in the WCPO, the terminal recruitments have often been fixed at the mean recruitment of the rest of the model period to reduce the instability that has been detected by retrospective analyses. This approach has been continued here with the terminal six recruitments fixed at the geometric mean, which is appropriate for a log-normally distributed random variable.

Spatially-aggregated (over all model regions) recruitment was assumed to have a weak relationship with annual mean spawning potential via a Beverton and Holt stock-recruitment relationship (SRR) with a fixed value of steepness ($h$). Steepness is defined as the ratio of the equilibrium recruitment produced by 20\% of the equilibrium unexploited spawning potential to that produced by the equilibrium unexploited spawning potential \citep{francis_use_1992, harley_preliminary_2011}. Typically, fisheries data are not very informative about the steepness parameter of the SRR parameters \citep{issf_report_2011}; hence, the steepness parameter was fixed at a moderate value (0.80) and the sensitivity of the model results to the value of steepness was explored by setting it to lower (0.65) and higher (0.95) values. The high CV (2.2) of the log-recruitment deviates, computed annually, ensured that the SRR had negligible impact on the estimation of recruitment and other model parameters, as recommended by \citet{ianelli_independent_2012}. The SRR was estimated over the period 1962--2020 to prevent the earlier recruitments (which may not be well estimated), and the terminal recruitments (which are not freely estimated), from influencing the relationship.

The SRR was incorporated mainly so that yield analysis and population projections could be undertaken for stock assessment purposes, particularly the determination of equilibrium- and depletion-based reference points. We therefore applied a weak penalty (equivalent to a CV of 2.2) for deviation from the SRR so that it would have negligible effect on recruitment and other model estimates \citep{hampton_spatially-disaggregated_2001}, but still allow the estimation of asymptotic recruitment. This approach was recommended (recommendation 20) by the 2011 bigeye assessment review \citep{ianelli_independent_2012}.

\subsubsection{Initial population}
\label{sec:model_initial_population}

The population age structure in the initial time period in each region was assumed to be in equilibrium. For this assessment the calculation of the equilibrium initial population was changed to no longer use the average total mortality (Z) for the first 20 time periods for the equilibrium initial population. Instead, for the initial equilibrium condition we applied an initial Z value equal to the natural mortality ($M$). The change to the settings for the initial population was introduced as part of the stepwise model development to implement the fully catch conditioned model. As noted above, the population is partitioned into quarterly age-classes with an aggregate class for the maximum age (plus-group). The aggregate age class makes it possible for accumulation of old and large fish, which is likely in the early years of the fishery when exploitation rates were very low.

\subsubsection{Growth}
\label{sec:model_growth}

The standard assumptions for WCPO assessments fitted in MFCL were made concerning age and growth: i) the lengths-at-age are normally distributed for each age-class; ii) the standard deviations of length for each age-class are a log-linear function of the mean lengths-at-age; and 3) the probability distributions of weights-at-age are a deterministic function of the lengths-at-age and a specified weight-length relationship\footnote{The length-weight relationship has been updated for the current assessment based on an analysis of current and historical port sampling data under WCPFC Project 90 \citep{macdonald_project_2023}}. These processes are assumed to be regionally and temporally invariant.

In the previous assessment several approaches to growth estimation were explored, including an external otolith based growth curve with a fixed Richards growth curve based on high-readability otoliths \citep{farley_age_2020}, a fixed external Richards growth curve based on the same high-readability otolith dataset but with the addition of tag-recapture growth increment data \citep{eveson_integrated_2020}, an internal MFCL  growth estimation using a conditional-age-at-length (CAAL) dataset from the otolith dataset including daily and annual ages \citep{farley_age_2020}, which also made use of the modal progressions apparent in various size composition data \citep{vincent_stock_2020}.

The assessment was sensitive to approaches used for growth and the internal CAAL estimate of the von Bertalanffy growth curve was the approach used for the diagnostic model.

The peer review of the yellowfin assessment \citep{punt_independent_2023} carefully considered the approaches to estimating growth. They concluded that based on how the otolith samples were selected according to length bins (i.e., attempting to achieve sufficient numbers of otolith samples for length bins across the length range, plus bias in otolith readability with age), that any external growth curves would likely be biased. They strongly recommended that for this type of otolith sampling regime the data should be used as CAAL data and the growth be estimated by the MFCL model if this is possible. We also note that in exploring the internal growth estimations we uncovered some features with the MFCL code for the internal Richards growth estimation that require attention, however, this could not be dealt with under the time constraints. For this assessment we use the CAAL and size composition data to estimate the von Bertalanffy growth curve internal to the MFCL model similar to the 2020 diagnostic model. When estimating the growth curve, similar to the previous assessment, we found the $L_1$ parameter tended to hit the lower bounds of 20 cm FL, so it was necessary to fix $L_1$ at 19.8 cm, consistent with the average size at the age of one quarter from the otoliths \citep{farley_age_2020}.

\subsubsection{Movement}
\label{sec:model_movement}

Movement was assumed to occur instantaneously at the beginning of each quarter via movement coefficients that connect regions sharing a common boundary. Note that fish can move between non-contiguous regions in a single time step due to the \enquote{implicit transition} computational algorithm employed (see \citealp{hampton_spatially-disaggregated_2001} and \citealp{kleiber_multifan-cl_2019} for details). Movement is parameterized by a pair of bi-directional coefficients at each region boundary. Movement is therefore possible in both directions across each regional boundary in each of the four quarters. Each of these coefficients is estimated independently resulting in 104 estimated movement parameters for the 9 region spatial structure and 76 for the 5 region spatial structure (adopted as the 2023 diagnostic structure) (2$\times$no.region boundaries (13)$\times$4 quarters). There are limited data from which to estimate long-term, annual variation in movement or age-specific movement rates. As such, the estimated seasonal pattern is assumed to be fixed across years and the movement coefficients are invariant with respect to age. A \enquote{prior} of 0 is assumed for all movement coefficients, and a low penalty is applied to deviations from the \enquote{prior}. We had hoped to explore different settings for modelling movement, as this is an area of uncertainty, however, this was not possible to do properly under time constraints.

\subsubsection{Natural mortality}
\label{sec:model_natmort}

The instantaneous rate of natural mortality ($M$) consists of an average over age classes and age-specific deviations from the average $M$. Average $M$ can be estimated internally by the model or specified as a fixed external value. Internal estimation can be constrained by providing a prior value and a penalty weight for deviations from the prior. Age specific deviations from average $M$ can also be estimated or input as a fixed  $M$-at-age function. The later approach was taken in the 2020 assessment for both bigeye and yellowfin \citep{ducharme-barth_stock_2020,vincent_stock_2020} where $M$-at-age was calculated using an approach applied to other tunas in the WCPO and EPO \citep{harley_simple_2003,hoyle_adjusted_2008,hoyle_sensitivity_2008}. The peer review had some difficulty with tracing the sources of data for some of the inputs to the external $M$-at-age function applied in 2020 that requires fitting a model that depends on empirical data on the sex-ratio at length, a growth curve, a base $M$ assumption for males, assumptions on critical lengths for inflections and a multiplier that determines the linear decline in $M$ for young ages to the base $M$, plus a length at which female mortality is assumed to begin to increase. There are uncertainties in all these empirical data and assumptions and the data for sex ratio is limited especially for larger and older fish. While the peer review generally supported the method, the construction of the external $M$-at-age function, notably the limitations in some of the available data  required to estimate it, the complexity of the calculations and the various assumption required are problematic. Furthermore, the external $M$-at-age function requires re-estimation if different growth curves are applied, further complicating its use. After the yellowfin peer review, a Tuna Stock Assessment Good Practices workshop run by the \enquote{Center for the Advancement of Population Assessment Methodology} (CAPAM) was held (March 2023, https://www.capamresearch.org/Tuna\_Stock\_Assessment\_Good\_Practices\_Workshop) and provided a strong endorsement for applying a simpler Lorenzen functional form for $M$-at-age estimation for tuna. Other CAPAM reviews of approaches for estimating $M$ have also favoured where possible estimating $M$ within the model, \enquote{let the data speak for themselves}, while being careful to review model diagnostics and plausibility of model estimated $M$ against external estimates \citep{hamel_natural_2023,punt_those_2023}.

In this assessment we chose to use the Lorenzen functional form for $M$-at-age (with $M$-at-age being inversely proportional to the mean length at age) and to estimate the asymptotic value of $M$, i.e. the $M$ for the oldest fish. The change from the previous external $M$-at-age to the Lorenzen form with internal $M$ estimation was made as part of the stepwise development of the 2023 diagnostic model.

\subsubsection{Reproductive potential}
\label{sec:model_reproductive_potential}

The reproductive potential ogive is an important component of the assessment structure as it translates model estimates of total population biomass to the relevant management quantity, spawning potential biomass (\sb). Assumptions about maturity do not affect the process of fitting the model, only the reference point values. The approach for calculating the reproductive potential at length ogive in this assessment is the same as the 2020 assessment \citep{vincent_stock_2020}. MFCL estimates the reproductive potential at age ogive internally from the externally calculated reproductive potential ogive at length. This length-based ogive is converted internally to the reproductive potential-at-age using a smooth-spline approximation \citep{davies_developments_2019}. This allows for a more natural definition of reproductive potential as the product of three length-based processes: proportion females-at-length\footnote{For the current assessment, female sex-ratio-at-length was calculated from Regional Observer Program data in SPC's holdings through to 2018, consistent with the previous assessment as few data have been collected since 2018.} (sex-ratio), proportion of females mature-at-length\footnote{Taken from \citet{farley_age_2017} as in the previous assessment.}, and the fecundity-at-length of mature females\footnote{Taken from \citet{sun_reproductive_2006} and standardized per kg of body weight at length as in the previous assessment.} (\autoref{fig:maturity_ogive}). Another added benefit is that this reproductive potential ogive is growth invariant. The previous stock assessments had to redefine the reproductive potential-at-age ogive for each different growth curve included in the assessment. As for the 2020 assessment we have not included spawning fraction in the reproductive potential calculation as the data are still not adequate for yellowfin in the WCPO.

\subsection{Fishery dynamics}
\label{sec:model_fihsery_dynamics}

\subsubsection{Selectivity}
\label{sec:model_selectivity}

Fishery selectivity coefficients at length and weight (for longline) are included in \autoref{fig:select_LL_and_index_wt} and \autoref{fig:select_other_length} and at age in \autoref{fig:select_LL_and_index_age} and \autoref{fig:select_other_age}. Selectivity was modeled using a cubic spline, as in the previous assessment. This allows for greater flexibility than assuming a functional relationship with age (e.g. logistic curve to model monotonically increasing selectivity or double-normal to model fisheries that select neither the youngest nor oldest fish), and requires fewer estimated parameters than modeling selectivity with separate age-specific coefficients. This is a form of smoothing, but the number of parameters for each fishery is the number of cubic spline `nodes' that are deemed sufficient to characterize selectivity over the age range. The number of nodes may vary among fisheries to allow for reasonably complex selectivity patterns, however we found that 5 nodes seemed sufficient for all fisheries in this case. Some other modifications to selectivity curves were made to better fit size composition data and these are noted in \autoref{sec:stepwise}.

In all cases, selectivity was assumed to be time-invariant and fishery-specific. However, a single selectivity function could be ``shared'' among a group of fisheries that have similar length compositions or were assumed to operate in a similar manner. This grouping facilitates a reduction in the number of parameters estimated and can provide insight into the regional abundance of fish of specific sizes. Selectivity groupings are indicated in \autoref{tab:fisheries_definitions}.

While length-based selectivity is not currently permitted in MFCL (Note: efforts to implement this feature in 2023 were made in response to a peer review recommendation but there was insufficient time to complete and test this feature), the age-based selectivity functions are penalized such that selectivity of age-classes that are similar in size will have similar selectivities for a given fishery or group of fisheries. Additionally, the assumption was made that at least one fishery within each of the model regions would have selectivity that was penalized to be increasing as a function of length in order to prevent the accumulation of an invulnerable, cryptic biomass within the model. In the previous assessment this was typically one of the extraction longline fisheries, though in region 2 (region 7 of the 9 region structure) it was assumed to be the Indonesia-Philippines handline fishery (Fishery 18), plus the index fisheries were also assumed to have selectivity penalised to be non-decreasing with length. However, in this assessment we chose not to penalize any of the extraction fisheries to have non-decreasing selectivity with increasing length and only apply this to the index fisheries which are assumed to share selectivity across regions (see further \autoref{sec:stepwise}).

\subsection{Dynamics of tagged fish}
\label{sec:model_tag_dynamics}

Tagged fish are modeled as discrete cohorts based on the region, year, quarter and age at release for the first 30 quarters after release. Subsequently, the tagged fish are pooled into a common group. This is to limit memory and computational requirements.

\subsubsection{Tag reporting}
\label{sec:reporting_rate}

In theory, tag-reporting rates can be estimated internally within the stock assessment model. In practice, experience has shown that independent information on tag-reporting rates for at least some fisheries tends to be required for reasonable model behavior to be obtained. We provided reporting rate priors for reporting groups (combinations for tagging programme and fishery) that reflect independent estimates of the reporting rates and their variance \citep{peatman_analysis_2023-2}. We also make some assumptions regarding fisheries that were similar to those with independent estimates, but increased the prior variance. For others where we felt there was very little information to inform priors and variance, uninformative priors were allocated, or in cases where there were 5 or less tag returns, those groups were not estimated. In these cases, the small numbers of tag returns were removed from the tagging data input file and the reporting rate for these groups set to a fixed value of zero. The prior reporting rates and penalty terms were informed from analyses of tag seeding experiments reported in \citet{peatman_analysis_2023-2}. For the RTTP and PTTP, relatively informative \enquote{priors} were formulated for the equatorial purse seine fisheries given that tag seeding experiments were focused on purse seiners.

All reporting rates were assumed to be time-invariant, and have an upper bound of 0.99 (increased from 0.9 in previous assessment, see \autoref{sec:stepwise}). For this assessment, as previously noted, we did not estimate reporting rates for tag reporting groups with five or few tag return. Tag recapture and reporting rate groupings are provided in \autoref{tab:tag_definitions}. Previous assessments have assumed fishery-specific reporting rates are constant over time. This assumption was reasonable when most of the tag data were associated with a single tagging program. However, tag reporting rates may vary considerably between tagging programs due to changes in the composition and operation of individual fisheries, and different levels of awareness and follow-up. Consequently, fishery-specific tag reporting rates that are also specific to individual tagging programs were estimated.

\subsubsection{Tag mixing}
\label{sec:tag_mixing}

The population dynamics of the fully recruited tagged and untagged populations are governed by the same model structures and parameters. The populations differ in respect of the recruitment process, which for the tagged population is the release of tagged fish, i.e., an individual tag and release event is the recruitment for that tagged population. Implicitly, we assume that the probability of recapturing a given tagged fish is the same as the probability of catching any given untagged fish in the same region and time period. For this assumption to be valid, either the distribution of fishing effort must be random with respect to tagged and untagged fish and/or the tagged fish must be randomly mixed with the untagged fish. The former condition is unlikely to be met because fishing effort is almost never randomly distributed in space. The second condition is also unlikely to be met soon after release because of insufficient time for mixing with the untagged population to have taken place.

Depending on the distribution of fishing effort in relation to tag release sites, the probability of capture of tagged fish soon after release may be different to that for the untagged fish. It is therefore desirable to designate one or more time periods after release as ``pre-mixed'' and compute fishing mortality for the tagged fish during this period based on the actual recaptures, corrected for tag reporting and tagging effects, rather than use fishing mortalities based on the general population parameters. This, in effect, desensitizes the likelihood function to tag recaptures in the specified ``pre-mixed'' periods while correctly removing fish from the tagged population that is present after the ``pre-mixed'' period. Herein we refer to the ``pre-mixed'' period as the ``mixing period''.

The allocation of appropriate ``mixing periods'' in stock assessments that use tag-recapture data is problematic as model estimations are sensitive to this assumption and misspecification can bias estimation of management quantities \citep{kolody_evaluation_2014}. Mixing rates may vary depending on release locations and times depending on various factors, including; oceanographic dynamics,  contexts of releases (e.g., FADs versus free schools, archipelagic waters versus oceanic), fishing effort distribution and environmental/food conditions that influence movements. The yellowfin peer review discussed mixing period assumptions and supported an approach applied to the 2022 skipjack assessment \citep{castillo_jordan_stock_2022,punt_independent_2023}. In this approach an external individual based model was used to estimate mixing periods 'specifically' for each release group, taking into account the locational and temporal varying conditions of each release event constituting the group that may result in different rates of mixing of the released fish \citep{scutt_phillips_quantifying_2022}. It applied the individual based Lagrangian model (Ikamoana) \citep{scutt_phillips_individual-based_2018} to simulate movement of individual fish (particles) and quantify the fishing pressure (observed) that individuals experienced across their dispersal trajectories in comparison to a population of simulated untagged particles. While this approach is promising, it requires substantial work to develop and it was not practical or possible to develop this approach for the current assessment, but could potentially be explored in future.

In the previous yellowfin assessments the diagnostic model assumed that tagged yellowfin gradually mix with the untagged population at the region-level and that this mixing process is complete by the end of the second quarter after the quarter in which the fish were releases (ie., ``mixing period'' assumption is two quarters). A sensitivities was include whereby the tag mixing period assumption was one quarter \citep{vincent_stock_2020}. Discussion at the PAW \citep{hamer_report_2023} recommended that external analysis of tag-recapture patterns would be useful to inform consideration of tag mixing assumptions for yellowfin as previous work on skipjack suggested that acceptable tag mixing may take longer than two quarters \citep{kolody_evaluation_2014}. In response to this the background analysis paper for this assessment includes a series of maps (see appendices, \citet{teears_cpue_2023}) that show tag release distributions and their related recapture distributions for individual tagging cruise and model regions at 0, 1, 2, 3 and 4 quarters after release (0 quarter meaning recapture with the same quarter of release, 1 quarter meaning recapture in the first quarter after release etc.). The recaptures are scaled to the purse seine catches (the purse fishery accounts for 94\% of yellowfin tag returns) and plotted as numbers of tags recaptured per 100 mt of catch in 1 x 1 degree grids cells. The maps provide spatial distributions of relative tag recapture rates within a model region, similar to the approach of \citet{langley_determining_2012}. When recaptures are observed in 1 x 1 cells spread throughout the model region in relation to the catch distribution and with roughly similar rates of recapture it could be considered a qualitative diagnostic that tag mixing was achieved. These plots were used to assign indicative mixing periods for fish released from the individual tag cruises and these were summarised to provide an indication of 'reasonable' mixing period assumptions to apply in sensitivity analyses \citep{teears_cpue_2023}. As further information to support this qualitative assessment, the distributions of displacement distances of tag recaptures at 1, 2, 3, 4, 5, and 6 quarters after release, for releases from model regions 3, 4, 7 and 8 of the 9 region model (see \citet{teears_cpue_2023}) were considered.

Overall, for tag releases with good numbers of recaptures these qualitative analyses supported mixing period sensitivities of 1, and 2 quarters \citep{teears_cpue_2023}, consistent with the previous assessment. Some tag releases were likely not mixed until at least 3 quarters, which could be considered as a third sensitivity, although perhaps less supported than 1 and 2 quarters. It was noted that some tag release groups showed more obvious evidence for mixing than others depending on how many tags were released (i.e., the qualitative assessments were likely more reliable for larger tag releases). While these qualitative assessments of mixing period assumption could  be improved with more time, they provide a stronger basis for mixing assumptions than in the previous assessment.

As per the 2020 assessment the tag return files were created using a sliding window to calculate the mixing period for each release group as per the recommendation of the 2020 PAW, which was further endorsed by the yellowfin assessment peer review \citep{punt_independent_2023}. This approach ensures that the mixing periods are applied faithfully in respect of actual times at liberty. It means that tags had to have a time at liberty of at least 182 days for a mixing period of 2 quarters as is assumed in the diagnostic case, or 91 days for a mixing period of 1 quarter.

\subsection{Likelihood components}
\label{sec:likelihood_components}

There are four data components that contribute to the log-likelihood function for the yellowfin stock assessment: the index fishery CPUE data, the length and weight frequency data, the tagging data and the conditional age at length data.

\subsubsection{Index fishery CPUE likelihood}
\label{sec:indexCPUE_likelihood}

In previous catch-errors models, abundance indices were constructed for extraction fisheries by assuming that catchability remained constant over time. In catch-conditioned models, a new approach has been implemented to model directly the CPUE for `index' fisheries. While such index fisheries exist in the model and in the data inputs as defined `fisheries', they differ from the regular `extraction' fisheries in that no catch is extracted and their CPUE is modelled directly as a lognormal likelihood contribution. The CPUE index series for each region is assigned a likelihood weight in the form of a region-specific $\sigma$, describing the magnitude of observation noise. The computational procedure for estimating $\sigma$ is based on maximum likelihood estimation, by calculating $\sigma$ as the standard deviation of log-residuals. The residuals are from a fitted model in the stepwise development where data weighting is adjusted. These maximum likelihood estimates of region-specific $\sigma$ are then kept the same throughout the stepwise development, diagnostic model, and structural uncertainty grid.

\subsubsection{Length and weight frequency}
\label{sec:sizefreq_likelihood}

The probability distributions for the length- and weight-frequency proportions are assumed to be approximated by robust normal distributions, with the variance determined by the effective sample size (ESS) and the observed length or weight-frequency proportion. Size frequency samples are assigned ESS lower than the number of fish measured. Lower ESS recognize that (i) length- and weight-frequency samples are not truly random (because of non-independence in the population with respect to size) and would have higher variance as a result; and (ii) the model does not include all possible process error, resulting in further under-estimation of variances. The observed sample sizes (OSS) are capped at 1,000 internal to MFCL, the sample size for the composition data used in the common index and extraction fisheries was divided by two to account if being used twice, and then the sample sizes were further divided by 20 for the diagnostic model, resulting in a 'maximum' ESS of 50 for each length or weight sample for a fishery. Alternative divisors for specifying ESS were explored in sensitivity analyses, these were 10 and 40. We further explored a self-scaling method for weighting the size composition date, the Dirichlet-multinomial (DM) likelihood \citep{thorson_model-based_2017}. The DM likelihood approach tended to put high weight on the size composition data leading to deterioration of the fits to the abundance indices. We therefore decided not to apply the DM likelihood weighting method, which requires further exploration. However, based on the testing of the DM weighting, the divisor of 10 would be more consistent with the DM weighting than the other two sensitivities.

\subsubsection{Tag data}
\label{sec:tagdata_likelihood}

A log-likelihood component for the tag data was computed using a negative binomial distribution. The negative binomial is preferred over the more commonly used Poisson distribution because tagging data often exhibit more variability than can be attributed by the Poisson. We employed a parameterization of the overdispersion parameter ($\tau$) such that as it approaches 1, the negative binomial approaches the Poisson. In the current assessment we assume $\tau=2$, which is a variance twice that of the Poisson. Therefore, if the tag return data show high variability (for example, due to contagion or non-independence of tags), then the negative binomial is able to recognize this. This should provide a more realistic weighting of the tag return data in the overall log-likelihood and allow the variability to impact the confidence intervals of estimated parameters.

\subsubsection{Conditional age-at-length data}
\label{sec:CAAL_likelihood}

A further log-likelihood component involves the conditional age-at-length (CAAL) dataset, the inclusion of which was recommended by the yellowfin tuna assessment peer review \citep{punt_independent_2023}. These data are included in the assessment to assist in estimating growth parameters because they provide direct observations of the distribution of fish ages within length classes. Each otolith sample was assigned to a corresponding length and age class in addition to a fishing incident based on the collection date of the sample and the gear by which it was captured. The model fits the observed age-at-length data along with information from size mode progression to influence the estimation of the growth curve. The observed age composition within each length interval is assumed to be multinomially distributed, and this forms the basis of the likelihood component for this data source. However, despite the otolith data being collected across a range of locations and times the CAAL data may have a degree of non-independence so we have downweighted the otolith samples for the diagnostic model by multiplying the input samples size by 0.75. We also ran sensitivities with downweighting of 0.5 and no downweighting (i.e,. 1). That latter is consistent with the 2020 yellowfin assessment.

\subsection{Parameter estimation and uncertainty}
\label{sec:parameter_uncertainty}

The parameters of the model were estimated by minimising the sum of the negative log-likelihoods associated with each of the data components plus the negative log of the probability density functions of the priors and penalties specified in the model. The optimisation to a specified gradient level or to a maximum number of function evaluations, if the specified gradient level is not met, was performed by an efficient optimization using exact derivatives with respect to the model parameters (auto-differentiation, \citet{fournier_ad_2012}). Estimation was conducted in a series of phases, the first of which used relatively arbitrary starting values for most parameters. A bash shell script, ``doitall'', implements the phased procedure for fitting the model. Some parameters were assigned specified starting values consistent with available biological information. The values of these parameters are provided in the .ini input file.

As a rule, models were run with a gradient criterion of $10^{-5}$ and a maximum of $10\,000$ function evaluations. This ensured that model runs would complete in less than 24 hours, allowing a daily routine of continuous model development. During the stepwise development, some model fits were later demonstrated to be at sub-optimal local minima when fits with improved objective functions were subsequently obtained by running several replicates with selected parameters perturbed, in a process referred to as jittering, and  continuing the optimisation process with additional function evaluations for each replicate. In many cases, the new jittered solution that had the best objective function value was a considerably different model fit in terms of derived management quantities. This was clearly cause for concern and required additional work to achieve parameter estimates that we could be confident had achieved a fit in terms of obtaining a good objective function value and reliable and stable estimates of the derived quantities of most interest for management advice. A thorough jittering process was therefore adopted for the diagnostic model and the structural uncertainty grid, selecting the model fit that has the best objective function value. These best fits may sometimes have worse gradients and Hessian properties than models at sub-optimal local minima. Thus, a positive definite Hessian (PDH) solution was less important in this process than considering the best value for the objective function and stable estimates of management quantities. We reached this conclusion after obtaining several apparently well-converged model solutions with PDH's that were subsequently improved, in some cases substantially, by jittering.

Additional steps were incorporated in a process adopted to find estimated model parameters with an improved objective function value, compared to solutions obtained without jittering. Due to the sensitivity of these models to initial conditions, an iterative process was adopted where key parameters for growth and mortality were reset close to values obtained from a previous run of the model. These values could either be reset at the initial phase or in an intermediate phase of the optimisation. This process was combined with repeated cycles of jittering, with up to 60 replicate jitters, and on occasions a secondary round of jittering on the best jittered solution found from the primary jitter. Typically, improvements in likelihood were routinely found in the primary round of jitters, sometimes with improvements to the objective function of tens to hundreds of units of log likelihood. Improvements in the secondary round of jittering (aka `twerking') could sometimes be found, but these were usually less than 10 units of log likelihood. Typically the secondary round of jittering produced little change to the stock status, but the primary jittering could produce changes of 5\% or more to the spawning biomass depletion stock status indicator. This process was used to select the 2023 diagnostic model and resulted in improvements to the objective function and overall likelihood. However, this added significant time to the assessment work. Once a stable `good fit' model was obtained, we calculated the Hessian. This process resulted in a well-converged, jittered diagnostic model with a PDH (see \autoref{sec:stepwise}).

The 2020 assessment ran a jitter analysis on the diagnostic model but did not find the same instability of solutions in relation to changes to the estimated stock status. This greater stability noted in 2020 may relate to that model using fixed (external) $M$-at-age and a fixed (externally estimated) growth curve, or perhaps there was insufficient time to investigate jittering more thoroughly.

This exhaustive process adopted to achieve a suitable diagnostic model in 2023, also meant that a single fit of a model in the structural uncertainty grid, while perhaps indicative, could not really be trusted, irrespective of the objective function value, gradient or Hessian status. While the grid models based off the diagnostic model are generally expected to have good starting parameter values, a jittering step for each model in the grid is now considered important to increase confidence that these models had also achieved stable solutions, both in terms of an improved objective function value and reliable management quantities. However, under time constraints the jittering of grid models was reduced to 20 jitters per model, with the Hessian calculated on the best jittered solution, where `best' is judged solely on the objective function values. The requirement to jitter and run Hessians on grid models again compounded the workload and computation time required for the assessment. However, this was all required to meet recommendations of the yellowfin peer review and the SC18 regarding model convergence.

Finally, we considered the SC18 concerns over a PDH being a mandatory diagnostic that needs to be achieved for a model to be included for management advice. We argue that while a PDH is desirable it is not as important as a stable `good fit' objective function and stable management quantities for these complex assessment models. Often the lack of PDH in these models with several thousand parameters estimated, is due to one or a few very small negative eigenvalues that have no influence on the estimations of the key management quantities. MFCL has advanced methods for computing estimation uncertainty for Hessians with negative eigenvalues, allowing the parameter estimation uncertainty to be calculated for Hessians with small numbers of small negative eigenvalues. We have shown by comparing pairs of similar models with the same structure, but with slightly different fits, one with and one without a PDH, the parameter estimation uncertainty is very similar (e.g., appendix 15.3 \citet{castillo_jordan_stock_2022}). While we expect that a PDH solution is an important diagnostic to meet for the diagnostic model, this criterion may not be essential (or practicable) for all grid models, especially if only a small number of small non-influential negative eigenvalues are found.

\textbf{Estimation uncertainty}

A positive definite Hessian is desirable to calculate parameter or estimation uncertainty for individual models, although as noted MFCL has advanced procedures for computing estimation uncertainty in the absence of a positive definite Hessian. Hessian calculations can take considerable computer time for these models, however, SC18 requested the Hessian status be reported for all models used for management advice (which includes all grid models). This has been completed in this assessment but has added significant time, and computational burden, leading to delays in finishing the assessment.

\textbf{Structural uncertainty}

The structural uncertainty grid attempts to describe the main sources of structural and data uncertainty in the assessment. Previous experience has shown that overall uncertainty is dominated by the structural uncertainty grid. For this assessment we have continued with a factorial grid of model runs which incorporates selected uncertainties explored in one-off sensitivity analyses.

The combined structural and estimation uncertainty is recommended to form the basis for assessing uncertainty and risk for the key stock status indicators.

\textbf{Likelihood profiles}

For highly complex population models fitted to large amounts of often conflicting data, it is common for there to be difficulties in estimating total abundance. Therefore, a likelihood profile analysis was undertaken of the marginal posterior likelihood in respect of population scaling, following the procedure outlined by \citet{mckechnie_stock_2017} and \citet{tremblay-boyer_stock_2017}. The results of these procedures are presented in the appendices (Appendices \autoref{sec:likelihood_profile}). Likelihood profiles are only presented for the diagnostic model.

Retrospective analyses were conducted as a general test of the stability of the model, as a robust model should produce similar output when rerun with data for the terminal quarters sequentially excluded \citep{cadigan_local_2005}. The retrospective analyses for the 2023 diagnostic model are presented in the appendices (Appendices \autoref{sec:retro_analysis}).

\subsection{Stock assessment interpretation methods}
\label{sec:stock_assessment_interp}

\subsubsection{Depletion and fishery impact}
\label{sec:methods_fishery_impacts}

Many assessments estimate the ratio of recent to initial biomass (usually spawning biomass) as an index of fishery depletion. The problem with this approach is that recruitment may vary considerably over the time series, and if either the initial or recent biomass estimates (or both) are \enquote{non-representative} because of recruitment variability or uncertainty, then the ratio may not measure fishery depletion, but simply reflect recruitment variability.

This problem is better approached by computing the spawning potential time series (at the model region level) using the estimated model parameters, but assuming that fishing mortality was zero. Because both the estimated spawning potential \sbt (with fishing), and the unexploited spawning potential \sbtfo, incorporate recruitment variability, their ratio at each quarterly time step ($t$) of the analysis, \sbtsbfo, can be interpreted as an index of fishery depletion. The computation of unexploited biomass includes an adjustment in recruitment to acknowledge the possibility of reduction of recruitment in exploited populations through stock-recruitment effects. To achieve this the estimated recruitment deviations are multiplied by a scalar based on the difference in the SRR between the estimated fished and unfished spawning potential estimates.

A similar approach can be used to estimate depletion associated with specific fisheries or groups of fisheries. Here, fishery groups of interest - longline, purse seine associated sets, purse seine unassociated sets, pole and line and \enquote{other} fisheries, are removed in-turn in separate simulations. The changes in depletion observed in these runs are then indicative of the depletion caused by the removed fisheries.

\subsubsection{Reference points}
\label{sec:methods_reference_points}

The unfished spawning potential (\sbfo) in each time period was calculated given the estimated recruitments and the Beverton-Holt SRR. This offers a basis for comparing the exploited population relative to the population subject to natural mortality only. The WCPFC adopted \lrp as a limit reference point (LRP) for the yellowfin stock where \sbfo for this assessment is calculated as the average over the period 2012--2021. There is no agreed WCPFC target reference point for the yellowfin tuna stock however CMM 2021-01 states in para 11 ``Pending agreement of a target reference point the spawning biomass depletion ratio (\sbfo) is to be maintained at or above the average \sbfo for 2012--2015''.  Stock status was referenced against these points by calculating the reference points; \sbrsbfo and \sblsbfo where \sbfo is calculated over 2011--2020 and \sbrecent and \sblatest are the mean of the estimated spawning potential over 2018--2021, and 2021 respectively (\autoref{tab:refpoints_table}).

The other key reference point, \fref, is the estimated average fishing mortality at the full assessment area scale over a recent period of time (\frecent; 2017--2020 for this stock assessment) divided by the fishing mortality producing MSY which is a product of the yield analysis and is detailed in \autoref{sec:method_yield}.

Several ancillary analyses using the converged model/s were conducted in order to interpret the results for stock assessment purposes. The methods involved are summarized below and the details can be found in \citet{kleiber_multifan-cl_2019}.

\subsubsection{Yield analysis}
\label{sec:method_yield}

The yield analysis consists of computing equilibrium catch (or yield) and spawning potential, conditional on a specified basal level of age-specific fishing mortality ($F_a$) for the entire model domain, a series of fishing mortality multipliers (\emph{fmult}), the natural mortality-at-age ($M_a$), the mean weight-at-age ($w_a$) and the SRR parameters. All of these parameters, apart from \emph{fmult}, which is arbitrarily specified over a range of 0--50 (in increments of 0.1), are available from the parameter estimates of the model. The maximum yield with respect to \emph{fmult} can be determined using the formulae given in \citet{kleiber_multifan-cl_2019}, and is equivalent to the MSY. Similarly, the spawning potential at MSY \sbmsy can be determined from this analysis. The ratios of the current (or recent average) levels of fishing mortality and spawning potential to their respective levels at MSY are determined for all models of interest. This analysis was conducted for all models in the structural uncertainty grid and thus includes alternative values of steepness assumed for the SRR.

Fishing mortality-at-age ($F_a$) for the yield analysis was determined as the mean over a recent period of time (2017--2020). We do not include 2021 in the average as fishing mortality tends to have high uncertainty for the terminal data year of the analysis and the catch and effort data for this terminal year are potentially incomplete. Additionally, recruitments for the terminal year of the model are constrained to be the geometric mean across the entire time series, which affects the $F$ for the youngest age classes.

MSY was also computed using the average annual $F_a$ from each year included in the model (1952--2021). This enabled temporal trends in MSY to be assessed and a consideration of the differences in MSY levels under historical patterns of age-specific exploitation.

\subsubsection{Kobe analysis and Majuro plots}
\label{sec:method_kobe}

For the standard yield analysis (\autoref{sec:method_yield}), the fishing mortality-at-age, $F_a$, is determined as the average over some recent period of time (2017--2020). In addition to this approach the MSY-based reference points (\ftfmsy), and \sbtsbmsy) were also computed by repeating the yield analysis for each year in turn. This enabled temporal trends in the reference points to be estimated and a consideration of the differences in MSY levels under historical patterns of age-specific exploitation. This analysis is presented in the form of dynamic Kobe plots and ``Majuro plots'', which have been presented for all stock assessments in recent years.

\subsubsection{Stock projections from the structural uncertainty grid}
\label{sec:method_projections}

Projections of stock assessment models can be conducted within MFCL to ensure consistency between the fitted models and the simulated future dynamics, and the framework for performing this exercise is detailed in \citet{pilling_approaches_2016}. Typically, stochastic 30 year projections of recent catch and effort (2019-2021) are conducted from each assessment model within the uncertainty grid developed. For each model, 100 stochastic projections, which incorporate future recruitments randomly sampled from historical deviates, are performed. The results of stock projections are included in the appendices (\autoref{sec:projections}).
